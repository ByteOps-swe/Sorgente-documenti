\section{Architettura di sistema}
\subsection{Architettura di implementazione}
Il sistema richiede la capacità di elaborare dati provenienti da diverse fonti in tempo reale e di fornire una visualizzazione immediata e continua di tali dati, permettendo di monitorarne gli andamenti e di rilevare eventuali anomalie. 
Per tale scopo, l'architettura di sistema adottata è la \textit{$\kappa$-architecture}.

\subsubsection{$\kappa$-architecture}
L'architettura Kappa è un modello di elaborazione dati in streaming che offre un'alternativa all'architettura Lambda. Il suo obiettivo principale è unificare l'elaborazione in tempo reale e batch (per i dati storici) all'interno di un unico stack tecnologico.
\paragraph{Vantaggi}
\begin{itemize}
    \item Semplice da implementare e gestire, costi di manutenzione ridotti;
    \item Assicura coerenza tra l'analisi in tempo reale e batch.
\end{itemize}
\paragraph*{Svantaggi}
\begin{itemize}
    \item Potenziale rallentamento dell'analisi in tempo reale, meno flessibile rispetto a Lambda.
\end{itemize}

\subsubsection{Componenti di sistema}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{../Images/SpecificaTecnica/Architettura_PB_microservices2.png}
    \caption{Componenti dell'architettura - Innovacity}
    \label{fig: fdf}
\end{figure}

\begin{itemize}
    \item \textbf{Data feed}:Le sorgenti dati sono costituite da sensori IoT dislocati sul territorio cittadino. Questi sensori sono in grado di inviare, ad intervalli regolari, messaggi contenenti misurazioni allo streaming layer;
    \item \textbf{Streaming layer}:Lo streaming layer gestisce i dati in arrivo in tempo reale, per poi archiviarli sistematicamente nello storage layer. Lo streaming Layer è composto da:
    \begin{itemize}
        \item \textbf{Apache Kafka}: Kafka è un sistema di messaggistica distribuito che consente di pubblicare, sottoscrivere e archiviare messaggi in tempo reale. Kafka è utilizzato per ricevere i dati dai sensori IoT e renderli disponibili per l'elaborazione in tempo reale e batch.
        \item \textbf{Clickhouse Kafka table engine}:consumatore che legge i
        dati dal server Kafka per persisterli nello storage layer.
    \end{itemize}
    \item \textbf{Processing Layer:} Il processing Layer è costituito da Faust che consuma i dati dallo streaming layer e li processa in tempo reale. Faust è un framework Python che consente di scrivere applicazioni di streaming in tempo reale. Faust è utilizzato per elaborare i dati in arrivo tramite un modello per il calcolo del punteggio di salute che poi viene reso nuovamente disponibili allo streaming layer.
    \item \textbf{Storage layer}:Lo storage layer è costituito da un database column-oriented, ClickHouse, che archivia i dati in arrivo dallo streaming layer. Questi dati sono disponibili per l'analisi e la visualizzazione in tempo reale e batch.
    \item \textbf{Data Visualization Layer}: composto da Grafana, si occupa della visualizzazione dei dati elaborati ottenuti dallo storage layer e della gestione delle notifiche in caso di anomalie rilevate.
\end{itemize}
\input{Sottosezioni/Specifica_tecnica/Architettura_simulatori.tex}
\input{Sottosezioni/Specifica_tecnica/Kafka_config.tex}
\input{Sottosezioni/Specifica_tecnica/Faust_processing.tex}

\input{Sottosezioni/Specifica_tecnica/Database_config.tex}
\input{Sottosezioni/Specifica_tecnica/Grafana_config.tex}
